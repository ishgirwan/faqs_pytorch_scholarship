# Introduction to Neural Networks

**Q: I am having some trouble understanding backpropagation when training the neural net.**

  Resource:

-   http://neuralnetworksanddeeplearning.com/chap2.html
-   https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec

**Q: What's the perceptron algorithm?**

  Resource:

-   https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53

**Q: How to find the optimal learning rate?**

  This paper by Leslie Smith is a great resource in finding the optimal learning rate: https://arxiv.org/abs/1803.09820 . You can find implementation of this paper in this blog: https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0

**Q: What is cross entropy Loss?**

  Resource:

-   https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a

**Q: What is bias?**

  Resource:

-  https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
-  https://www.youtube.com/watch?v=aircAruvnKk

**Q: What is Gradient Descent?**

  Resource:

-  https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s

**Q: In softmax function why do we take exponential?**

  Resource:

-  https://datascience.stackexchange.com/questions/23159/in-softmax-classifier-why-use-exp-function-to-do-normalization

**Q: Why do we need activation function?**

  Resource:
  
-  https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f
        
