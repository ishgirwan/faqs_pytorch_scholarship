# Introduction to Neural Networks

**Q:I am having some trouble understanding backpropagation when training the neural net.**

Resources:

- http://neuralnetworksanddeeplearning.com/chap2.html
- https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec

**Q:What's the perceptron algorithm?**

Resources:

- https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53

**Q:How to find the optimal learning rate?**

This paper by Leslie Smith is a great resource in finding the optimal learning rate: https://arxiv.org/abs/1803.09820 . You can find implementation of this paper in this blog:https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0

**Q:What is cross entropy Loss?**

Resources:

-https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a

**Q:What is bias?**

Resources:

-https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
-https://www.youtube.com/watch?v=aircAruvnKk

**Q:What is Gradient Descent?**

Resources:

-https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s

**Q:In softmax function why do we take exponential?**

Resources:

-https://datascience.stackexchange.com/questions/23159/in-softmax-classifier-why-use-exp-function-to-do-normalization


        
